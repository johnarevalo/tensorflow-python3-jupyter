{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN definition\n",
    "\n",
    "In a language model we want to estimate the joint probability of a sequence of symbols (words, chars, phonemes, etc):\n",
    "$$ P\\left(x_{1},x_{2},\\dots,x_{T}\\right) = \\prod_{t=1}^{T}P\\left(x_{t}|x_{1},\\dots,x_{t-1}\\right) $$\n",
    "\n",
    "A Recurrent Neural Language Model models the conditional probability as:\n",
    "\n",
    "$$ P\\left(x_{t}|x_{1},\\dots,x_{t-1}\\right) = f_\\Theta\\left(x_{t}, h_{t - 1} \\right) $$\n",
    "\n",
    "Where $f_\\Theta $ represents the Neural network transformations and $ \\Theta $ the set of parameters to be learned. $ h_t $ is the hidden state at time $ t $. The $ h_0 $ is part of the learned parameters in the model ($ h_0 \\in \\Theta $). \n",
    "\n",
    "With a rnn char-based language model, the neural netork architecture would be:\n",
    "\n",
    "![alt text](unroll.jpeg \"Unrolled network\")\n",
    "\n",
    "Here $ p_t \\in \\mathbb{R}^{\\left| V \\right|} $ is the probability distribution of the next character. It contains as many values as characters we have in the vocabulary (i.e. $ \\left| V \\right| $)\n",
    "\n",
    "## Playing with a RNN model\n",
    "\n",
    "This notebook provides a wrapper class for a Language model trained with a small [Shakespeare corpus](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). You can see this class as the object representing the gray circle in the previous figure. It contains the function ```predict(char, state=h_0)``` which receives the input char $ c_t $ and (optionally) the previous state $ h_{t-1} $, and outputs the probability distribution $ p_t $ and the hidden state $ h_t $. If the previous state is not given, the default initial state $ h_0 $ is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "import cufflinks as cf\n",
    "import pandas as pd\n",
    "\n",
    "cf.set_config_file(offline=True, world_readable=True)\n",
    "\n",
    "\n",
    "corpus = './corpus1'\n",
    "\n",
    "class Sampler(object):\n",
    "    def __init__(self):\n",
    "        with open(os.path.join(corpus, 'data', 'char_to_idx.json')) as f:\n",
    "            self.char_to_idx = json.load(f)\n",
    "        self.idx_to_char = { i: ch for (ch, i) in self.char_to_idx.items() }\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocab_size, 512, batch_input_shape=(1, 1)))\n",
    "        for i in range(3):\n",
    "            model.add(LSTM(256, return_sequences=(i != 2), stateful=True))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(self.vocab_size))\n",
    "        model.add(Activation('softmax'))\n",
    "        self.model = model\n",
    "    \n",
    "\n",
    "    def sample(self, num_chars, header='', epoch=100, returnProb=False):\n",
    "        self.model.load_weights(os.path.join(corpus, 'model', 'weights.{}.h5'.format(epoch)))\n",
    "\n",
    "        sampled = [self.char_to_idx[c] for c in header]\n",
    "        for c in header[:-1]:\n",
    "            batch = np.zeros((1, 1))\n",
    "            batch[0, 0] = self.char_to_idx[c]\n",
    "            self.model.predict_on_batch(batch)\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            batch = np.zeros((1, 1))\n",
    "            if sampled:\n",
    "                batch[0, 0] = sampled[-1]\n",
    "            else:\n",
    "                batch[0, 0] = np.random.randint(self.vocab_size)\n",
    "            prob = self.model.predict_on_batch(batch).ravel()\n",
    "            sample = np.random.choice(range(self.vocab_size), p=prob)\n",
    "            sampled.append(sample)\n",
    "\n",
    "        if returnProb:\n",
    "            return ''.join(self.idx_to_char[c] for c in sampled), prob\n",
    "        else:\n",
    "            return ''.join(self.idx_to_char[c] for c in sampled)\n",
    "\n",
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' p', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar', size=(100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' pe', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' per', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' perd', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' perdo', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, prob = sampler.sample(1, header=' perdon', returnProb=True)\n",
    "dist = pd.Series(prob, index=[sampler.idx_to_char[ix] for ix in range(sampler.vocab_size)])\n",
    "dist.sort_values(ascending=False).iloc[:20].iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.sample(1000, header='y jes√∫s dijo:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else to do with language models:\n",
    " - Complete missing characters:\n",
    "   B i e n _ v e n t _ r a d o ...\n",
    " - Given a set of characters, what is the most likely word:\n",
    "   {m,r,a,o}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
